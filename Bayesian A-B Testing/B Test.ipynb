{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A/B Test Analysis with Bayesian Statistics\n",
    "This notebook performs an end-to-end analysis of A/B test results using Bayesian statistical methods. The objective is to go beyond the traditional \"p-value\" to calculate the probability of each variant being the best and to quantify the expected risk associated with choosing one variant over another."
   ],
   "id": "abf4300b80bcbe16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "The next two cells begins by setting up the environment. First, it imports the essential Python libraries: pandas for data manipulation, numpy for numerical operations, scipy.stats for statistical functions, and matplotlib/seaborn for visualization.\n",
    "\n",
    "Next, it defines the core parameters for the analysis:\n",
    "\n",
    "- FILE_NAME: The name of the input data file.\n",
    "\n",
    "- PRIOR_ALPHA & PRIOR_BETA: The initial parameters for our Bayesian model, set to 1.0 to represent an uninformative prior.\n",
    "\n",
    "- RISK_THRESHOLD: The pre-defined risk tolerance that will be used to make the final decision.\n",
    "\n",
    "The main objective is to use this setup to go beyond the traditional \"p-value\" to calculate the probability of each variant being the best and to quantify the expected risk associated with choosing one variant over another."
   ],
   "id": "93851f08a4e7811b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "a427fe66ce8fc60d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Global Parameter Definition\n",
    "\n",
    "# This cell centralizes all the key parameters and constants that will be used\n",
    "# throughout the notebook. This makes it easy to adjust the analysis settings\n",
    "# without searching through the entire code.\n",
    "\n",
    "# --- File Configuration ---\n",
    "# The name of the Excel file containing the A/B test data.\n",
    "# The project description specifies that this file should be in the same\n",
    "# directory as the notebook.\n",
    "FILE_NAME = 'data_ab.xlsx'\n",
    "\n",
    "\n",
    "# --- Bayesian Prior Configuration ---\n",
    "# These parameters define our prior belief about the conversion rate before\n",
    "# observing any data.\n",
    "#\n",
    "# By setting alpha and beta to 1.0, we are using a Beta(1, 1) distribution,\n",
    "# which is mathematically equivalent to a Uniform(0, 1) distribution. This is a\n",
    "# common \"uninformative prior\" used when we have no strong initial assumptions\n",
    "# about what the conversion rate might be.\n",
    "PRIOR_ALPHA = 1.0\n",
    "PRIOR_BETA = 1.0\n",
    "\n",
    "\n",
    "# --- Decision Framework Configuration ---\n",
    "# This defines the risk tolerance for making a decision. The Expected Loss\n",
    "# (the risk) of choosing a variant will be compared against this threshold.\n",
    "#\n",
    "# A value of 0.01 means we will only recommend a winning variant if the\n",
    "# potential downside of making that choice is less than 1% of the conversion rate.\n",
    "# This value establishes a clear, pre-defined rule for the final recommendation.\n",
    "RISK_THRESHOLD = 0.01"
   ],
   "id": "5a1de2caa799e5b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Loading and Validation\n",
    "This cell handles the critical first step of loading and validating the A/B test data. The code will read the specified Excel file, confirm its existence, and verify that it contains the necessary columns for the analysis: variante, visitantes, and conversoes. This ensures the data is correctly structured before proceeding."
   ],
   "id": "1b54c2311f245adc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data Loading and Validation\n",
    "import os # The 'os' library is used here to check for file existence\n",
    "\n",
    "# --- Define Expected Data Structure ---\n",
    "# These are the column names the script expects to find in the Excel file.\n",
    "# The analysis logic depends on these specific columns being present.\n",
    "REQUIRED_COLUMNS = ['variante', 'visitantes', 'conversoes']\n",
    "\n",
    "print(f\"Attempting to load data from '{FILE_NAME}'...\")\n",
    "\n",
    "# --- Validation and Loading Logic ---\n",
    "# First, check if the file exists in the directory.\n",
    "if not os.path.exists(FILE_NAME):\n",
    "    # If the file is not found, print a clear error and stop.\n",
    "    print(f\"---\")\n",
    "    print(f\"Error: File Not Found.\")\n",
    "    print(f\"The data file '{FILE_NAME}' was not found in the current directory.\")\n",
    "    print(f\"Please ensure the file is present before proceeding.\")\n",
    "\n",
    "else:\n",
    "    # If the file exists, try to load and validate it.\n",
    "    try:\n",
    "        # Load the Excel file into a pandas DataFrame.\n",
    "        df = pd.read_excel(FILE_NAME)\n",
    "\n",
    "        # Check if all the required columns are in the DataFrame.\n",
    "        if all(col in df.columns for col in REQUIRED_COLUMNS):\n",
    "            # If validation is successful, print a confirmation and the data's head.\n",
    "            print(\"Success: File loaded and validated.\")\n",
    "            print(\"\\n--- First 5 Rows of the Dataset ---\")\n",
    "            # Display the first 5 rows for a quick visual inspection.\n",
    "            # Using display() is ideal for Jupyter notebooks for better formatting.\n",
    "            display(df.head())\n",
    "        else:\n",
    "            # If columns are missing, identify them and report the error.\n",
    "            missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
    "            print(f\"---\")\n",
    "            print(f\"Error: Missing Columns.\")\n",
    "            print(f\"The file '{FILE_NAME}' was loaded, but is missing required columns.\")\n",
    "            print(f\"Missing column(s): {missing_cols}\")\n",
    "            print(f\"Please ensure the file contains all of the following columns: {REQUIRED_COLUMNS}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other potential errors during the file reading process.\n",
    "        print(f\"---\")\n",
    "        print(f\"An unexpected error occurred while reading the file: {e}\")"
   ],
   "id": "5f1361b781a403a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Calculation of Posterior Parameters\n",
    "Here, we perform the core Bayesian update for our A/B test. 🧪\n",
    "\n",
    "This cell calculates the **posterior parameters** (`alpha` and `beta`) for each variant by combining our initial **prior beliefs** with the observed data. The formulas are straightforward:\n",
    "\n",
    "-   `posterior_alpha = prior_alpha + conversoes`\n",
    "-   `posterior_beta = prior_beta + (visitantes - conversoes)`\n",
    "\n",
    "These new parameters define the posterior Beta distribution for each variant, representing our updated understanding of the true conversion rate after seeing the data."
   ],
   "id": "abd7b472985bcbb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculation of Posterior Parameters\n",
    "\n",
    "# This cell applies the core Bayesian update rule for the Beta-Binomial model.\n",
    "# The 'posterior' distribution represents our updated belief about the conversion\n",
    "# rate after incorporating the evidence from the collected data.\n",
    "\n",
    "# For a Beta-Binomial model, the posterior parameters are calculated as follows:\n",
    "# posterior_alpha = prior_alpha + number_of_successes (conversions)\n",
    "# posterior_beta = prior_beta + number_of_failures (visitors - conversions)\n",
    "\n",
    "# Check if the dataframe 'df' exists to prevent errors if previous cells failed.\n",
    "if 'df' in locals():\n",
    "    # Calculate the posterior alpha parameter for each variant.\n",
    "    df['posterior_alpha'] = PRIOR_ALPHA + df['conversoes']\n",
    "\n",
    "    # Calculate the posterior beta parameter for each variant.\n",
    "    df['posterior_beta'] = PRIOR_BETA + (df['visitantes'] - df['conversoes'])\n",
    "\n",
    "\n",
    "    # --- Display the Updated DataFrame ---\n",
    "    # Show the DataFrame with the newly calculated posterior parameters.\n",
    "    # These parameters fully define the probability distributions for the\n",
    "    # conversion rate of each variant.\n",
    "\n",
    "    print(\"--- DataFrame with Posterior Parameters ---\")\n",
    "    display(df)\n",
    "\n",
    "else:\n",
    "    print(\"Error: The DataFrame 'df' does not exist.\")\n",
    "    print(\"Please ensure the data loading and validation cell (Cell 5) was executed successfully.\")"
   ],
   "id": "34600db8b6eeee06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Generation of the Posterior Plot\n",
    "\n",
    "This cell generates the most important visualization for our analysis: the posterior probability distributions. 📊\n",
    "\n",
    "The code plots a curve for each variant, representing our final belief about its true conversion rate. This allows for a direct visual comparison. Pay attention to two key aspects of the curves:\n",
    "\n",
    "-   **Position**: A curve shifted to the right indicates a higher probable conversion rate.\n",
    "-   **Shape**: A taller, narrower curve signifies greater certainty about the conversion rate's value."
   ],
   "id": "339d27af55852f67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generation of the Posterior Plot\n",
    "\n",
    "# This cell visualizes the posterior distributions for each variant.\n",
    "# Each curve represents the range of probable values for the true conversion\n",
    "# rate of a variant, given the data. Taller, narrower curves indicate more\n",
    "# certainty about the conversion rate.\n",
    "\n",
    "# Check if the dataframe 'df' with posterior columns exists to prevent errors.\n",
    "if 'df' in locals() and 'posterior_alpha' in df.columns:\n",
    "\n",
    "    # --- 1. Setup for Plotting ---\n",
    "    # Create a figure to hold the plot.\n",
    "    plt.figure(figsize=(12, 7))\n",
    "\n",
    "    # To make the plot robust, we dynamically determine a suitable range for the x-axis.\n",
    "    # We'll find the 99.9th percentile of each distribution and set the limit\n",
    "    # just beyond the maximum of these to ensure all curves are fully visible.\n",
    "    max_x = 0\n",
    "    for i, row in df.iterrows():\n",
    "        # Calculate the value at which 99.9% of the distribution is covered\n",
    "        percentile_999 = stats.beta.ppf(0.999, row['posterior_alpha'], row['posterior_beta'])\n",
    "        if percentile_999 > max_x:\n",
    "            max_x = percentile_999\n",
    "\n",
    "    # Create a smooth range of x-values (conversion rates) for plotting the curves.\n",
    "    # We add 10% padding to the max value for better visualization.\n",
    "    x = np.linspace(0, max_x * 1.1, 1000)\n",
    "\n",
    "    # --- 2. Plot Each Variant's Posterior Distribution ---\n",
    "    # Iterate over each variant (row) in the DataFrame.\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the parameters and name for the current variant.\n",
    "        variant_name = row['variante']\n",
    "        p_alpha = row['posterior_alpha']\n",
    "        p_beta = row['posterior_beta']\n",
    "\n",
    "        # Calculate the Probability Density Function (PDF) for the Beta distribution.\n",
    "        pdf = stats.beta.pdf(x, p_alpha, p_beta)\n",
    "\n",
    "        # Plot the PDF curve with a label for the legend.\n",
    "        plt.plot(x, pdf, label=f'Variant {variant_name}')\n",
    "\n",
    "\n",
    "    # --- 3. Finalize and Display the Plot ---\n",
    "    # Add a clear title and labels as specified in the project description.\n",
    "    plt.title('Posterior Probability Distributions of Conversion Rates', fontsize=16)\n",
    "    plt.xlabel('Conversion Rate', fontsize=12)\n",
    "    plt.ylabel('Probability Density', fontsize=12)\n",
    "\n",
    "    # Display the legend to identify which curve corresponds to which variant.\n",
    "    plt.legend()\n",
    "\n",
    "    # Improve layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Render the plot.\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Error: DataFrame 'df' with posterior parameters not found.\")\n",
    "    print(\"Please ensure the data loading (Cell 5) and posterior calculation (Cell 7) were executed successfully.\")"
   ],
   "id": "daafde3df8f8301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. # Monte Carlo Simulation\n",
    "While the plot provided a visual comparison, we need to quantify the results to make a decision. To do this, we'll run a **Monte Carlo simulation**. 🎰\n",
    "\n",
    "The code below draws a large number (`100,000`) of random samples from each variant's posterior Beta distribution. Each sample can be thought of as a plausible \"true conversion rate\" for that variant, according to our model.\n",
    "\n",
    "By creating these large sets of simulated outcomes, we can directly compare them to calculate precise metrics in the following steps, such as the probability of one variant being superior to another."
   ],
   "id": "236e80b258b92449"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Monte Carlo Simulation\n",
    "\n",
    "# This cell performs a Monte Carlo simulation to draw random samples from the\n",
    "# posterior distribution of each variant. These samples will allow us to\n",
    "# empirically compare the variants and calculate key metrics, such as the\n",
    "# probability of one being better than the other and the expected loss.\n",
    "\n",
    "# --- 1. Simulation Setup ---\n",
    "# Define the number of random samples to generate for each variant's distribution.\n",
    "# A larger number of samples leads to more stable and accurate estimates of our metrics.\n",
    "N_SAMPLES = 100000\n",
    "\n",
    "# We will store the generated samples in a dictionary, with variant names as keys.\n",
    "posterior_samples = {}\n",
    "\n",
    "\n",
    "# --- 2. Run Simulation ---\n",
    "# Check if the dataframe 'df' with posterior parameters exists to avoid errors.\n",
    "if 'df' in locals() and 'posterior_alpha' in df.columns:\n",
    "\n",
    "    # Iterate over each variant (row) in the DataFrame.\n",
    "    for i, row in df.iterrows():\n",
    "        variant_name = row['variante']\n",
    "        p_alpha = row['posterior_alpha']\n",
    "        p_beta = row['posterior_beta']\n",
    "\n",
    "        # Generate N_SAMPLES from the Beta distribution defined by the variant's\n",
    "        # posterior parameters. Each sample represents a plausible \"true\"\n",
    "        # conversion rate for that variant, according to our model.\n",
    "        samples = stats.beta.rvs(a=p_alpha, b=p_beta, size=N_SAMPLES)\n",
    "\n",
    "        # Store the resulting array of samples in our dictionary.\n",
    "        posterior_samples[variant_name] = samples\n",
    "\n",
    "    # --- 3. Output: Simulation Summary ---\n",
    "    # The simulation is complete. The following is a summary of the process.\n",
    "    print(\"--- Monte Carlo Simulation Summary ---\")\n",
    "    print(f\"✅ Simulation completed successfully.\")\n",
    "    print(f\"   - Samples generated per variant: {N_SAMPLES:,}\")\n",
    "    print(f\"   - Variants simulated: {list(posterior_samples.keys())}\")\n",
    "\n",
    "    print(\"\\nData Preview (first 3 samples for each variant):\")\n",
    "    for variant, samples in posterior_samples.items():\n",
    "        preview = [round(s, 6) for s in samples[:3]]\n",
    "        print(f\"  - {variant}: {preview}\")\n",
    "\n",
    "    print(\"\\nThe 'posterior_samples' dictionary is now ready for metric calculation in the next cell.\")\n",
    "\n",
    "else:\n",
    "    # This message will only be displayed if the prerequisite DataFrame is not found.\n",
    "    print(\"Error: DataFrame 'df' with posterior parameters not found.\")\n",
    "    print(\"Please ensure the data loading (Cell 5) and posterior calculation (Cell 7) were executed successfully.\")"
   ],
   "id": "fb15b530dd5c025d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Calculation and Presentation of Metrics\n",
    "This is the final calculation step where we translate our simulation results into actionable business metrics. 🏆\n",
    "\n",
    "The code below uses the thousands of simulated outcomes to compute two key metrics for each variant:\n",
    "\n",
    "1.  **Probability of Being Best**: The percentage of simulations where a variant had the highest conversion rate. This answers the question: \"Which variant is most likely the winner?\"\n",
    "2.  **Expected Loss (Risk)**: The average \"regret\" or loss we would incur by choosing a variant if another one was actually better. This answers the question: \"What is the cost of being wrong?\"\n",
    "\n",
    "The results will be sorted by risk, providing a clear recommendation for which variant is the optimal choice from a risk-management perspective."
   ],
   "id": "667f4aa713eb94f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculation and Presentation of Metrics\n",
    "\n",
    "# This cell uses the simulated samples to calculate the final decision metrics\n",
    "# for any number of variants.\n",
    "\n",
    "# Check if the 'posterior_samples' dictionary exists and is populated.\n",
    "if 'posterior_samples' in locals() and len(posterior_samples) >= 1:\n",
    "\n",
    "    # --- 1. Consolidate All Samples ---\n",
    "    variant_names = list(posterior_samples.keys())\n",
    "    # Stack all sample arrays into a single 2D NumPy array for efficient calculation\n",
    "    # Shape will be (number_of_variants, N_SAMPLES)\n",
    "    all_samples_array = np.stack([posterior_samples[name] for name in variant_names])\n",
    "\n",
    "    # --- 2. Calculate Key Metrics for Each Variant ---\n",
    "\n",
    "    # Find the best performing variant in each of the N_SAMPLES simulations\n",
    "    best_variant_samples = all_samples_array.max(axis=0)\n",
    "\n",
    "    # Find the index (0, 1, 2, etc.) of the winning variant in each simulation\n",
    "    best_variant_indices = all_samples_array.argmax(axis=0)\n",
    "\n",
    "    results = []\n",
    "    # Now, loop through each variant to calculate its specific metrics\n",
    "    for i, name in enumerate(variant_names):\n",
    "\n",
    "        # i) Probability of Being Best\n",
    "        # The proportion of simulations where this variant was the winner\n",
    "        prob_is_best = np.mean(best_variant_indices == i)\n",
    "\n",
    "        # ii) Expected Loss (Risk)\n",
    "        # The average loss incurred if we choose this variant, but another was better\n",
    "        samples_for_this_variant = all_samples_array[i]\n",
    "        expected_loss = np.mean(best_variant_samples - samples_for_this_variant)\n",
    "\n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"prob_best\": prob_is_best,\n",
    "            \"expected_loss\": expected_loss\n",
    "        })\n",
    "\n",
    "    # --- 3. Format and Display Results ---\n",
    "    print(\"--- Bayesian A/B Test Results ---\")\n",
    "    print(\"Metrics calculated for each variant:\\n\")\n",
    "\n",
    "    # Sort results by the lowest risk for clearer presentation\n",
    "    sorted_results = sorted(results, key=lambda x: x['expected_loss'])\n",
    "\n",
    "    for result in sorted_results:\n",
    "        print(f\"Variant: '{result['name']}'\")\n",
    "        print(f\"  - Probability of being the best: {result['prob_best']:.1%}\")\n",
    "        print(f\"  - Risk (Expected Loss): {result['expected_loss']:.6f}\")\n",
    "        print(\"-\" * 35)\n",
    "\n",
    "    print(\"(Risk is the average amount you stand to lose by choosing this variant if another one was actually better.)\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Error: The 'posterior_samples' dictionary is not available.\")\n",
    "    print(\"Please ensure the Monte Carlo simulation (Cell 11) was executed successfully.\")"
   ],
   "id": "c4fd4446b762dcdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Automated Conclusion Logic\n",
    "This final, automated cell translates our statistical results into a clear business recommendation. 🎯\n",
    "\n",
    "The logic is straightforward:\n",
    "1.  It takes our best variant (the one with the lowest **Expected Loss**).\n",
    "2.  It compares its risk level against a predefined **Risk Threshold** (`RISK_THRESHOLD`).\n",
    "\n",
    "Based on this comparison, the code will either give a \"go\" signal to implement the winning variant or recommend that the test is inconclusive because even the best option is too risky to deploy."
   ],
   "id": "84061d34cea7335d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Automated Conclusion Logic\n",
    "\n",
    "# This final cell takes the sorted list of results and uses the single\n",
    "# best-performing variant to make a final recommendation.\n",
    "\n",
    "print(\"--- Automated Recommendation ---\")\n",
    "\n",
    "# We use a try-except block to provide a more specific error message.\n",
    "try:\n",
    "    # --- Decision Framework ---\n",
    "    print(f\"Decision Threshold (Max Acceptable Risk): {RISK_THRESHOLD:.4f}\\n\")\n",
    "\n",
    "    # The results from Cell 12 are already sorted by lowest risk (Expected Loss).\n",
    "    # We only need to evaluate the top candidate.\n",
    "    best_candidate = sorted_results[0]\n",
    "    best_variant_name = best_candidate['name']\n",
    "    lowest_risk = best_candidate['expected_loss']\n",
    "\n",
    "    # Compare the risk of the best variant against the threshold.\n",
    "    if lowest_risk < RISK_THRESHOLD:\n",
    "        print(f\"✅ Recommendation: Implement Variant '{best_variant_name}'.\")\n",
    "        print(f\"Justification: This variant has the lowest risk ({lowest_risk:.6f}), which is below the acceptable threshold.\")\n",
    "\n",
    "    # If even the best variant is too risky.\n",
    "    else:\n",
    "        print(f\"⚠️ Recommendation: The test is inconclusive.\")\n",
    "        print(f\"Justification: Even the best variant, '{best_variant_name}', has a risk ({lowest_risk:.6f}) that is higher than your threshold.\")\n",
    "        print(\"Consider collecting more data to reduce uncertainty.\")\n",
    "\n",
    "except NameError as e:\n",
    "    # This block runs ONLY if a required variable (like 'sorted_results' or 'RISK_THRESHOLD') was not found.\n",
    "    print(f\"\\n--- ERROR ---\")\n",
    "    print(f\"Could not generate a recommendation because a required variable is missing.\")\n",
    "    print(f\"Specific Error: {e}\")\n",
    "    print(\"\\nThis confirms that a variable from a previous cell was not created successfully.\")\n",
    "    print(\"Please check the output of Cell 3 and Cell 12 to ensure they ran without issues.\")"
   ],
   "id": "62365a9c7c89e4db",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
